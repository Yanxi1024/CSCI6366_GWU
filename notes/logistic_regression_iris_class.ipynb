{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Logistic Regression From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loading our test data\n",
    "We're going to use the iris dataset for our example. It is a collection of stem and petal measurements for three different types of irises. The objective of this dataset is to identify the type of iris based on the iris's measurements.\n",
    "\n",
    "<div style=\"margin:auto;width:50%\">\n",
    "\n",
    "<img src=\"versicolor.jpg\" style=\"max-width: 30rem;\"/>\n",
    "\n",
    "<p>Photo taken by Danielle Langlois in July 2005 at the Forillon National Park of Canada, Quebec, Canada. CC-BY-SA</p>\n",
    "\n",
    "</div>\n",
    "\n",
    "----\n",
    "## Setting up our expirement\n",
    "\n",
    "To keep our example simple, we'll limit our test data to:\n",
    " - Two features or measurements\n",
    "   - This to make it easier to visaulize our data\n",
    " - Two classes\n",
    "   - This way we can focus on binary classification as multi-class classification requires a more complex model (or multiple models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iris dataset is built into the scikit-learn library, which the above function leverages. This loads the data into a custom object type, but we'll just focus on the `data` or **independent variables** and `target` or **dependent variables**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below logic states to take : (all rows), :2 (first two columns)\n",
    "feats = iris_data['data'][:,:2]\n",
    "class_labels = iris_data['target']\n",
    "\n",
    "X = feats\n",
    "y = class_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing our dataset to two classes\n",
    "The iris dataset contains 3 types of irises: setosa, versicolor, virginica. However, logistic regression is a binary classification system. We will convert the three-class data set to two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(iris_data['target_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our target variable (y) has three values [0, 1, 2]. By merging classes 1 and 2, we can turn this in to a binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([0 if i == 0 else 1 for i in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot our test data\n",
    "Lets plot the test data to verify everything is in order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['red' if x == 0 else 'blue' for x in y]\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(X[:, 0], X[:, 1], c = colors, alpha = 0.5)\n",
    "plt.title(\"Iris Types by Stem and Petal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep and Initailize Our Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Lets randomly split things into training and testing sets so we don't cheat\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Support Functions\n",
    "To build our logistic model we'll need a couple of support functions:\n",
    " - **sigmoid**: The final stage of the computational graph requires taking the logistic function of the outputs\n",
    "$$\\frac{1}{1+ e^{-z}} $$\n",
    " \n",
    " - **scores**: Weights times inputs\n",
    " \n",
    "$$ z = \\vec{w} \\cdot \\vec{x} $$\n",
    " - **log_likelihood**: The loss function for logistic regression\n",
    "   - Recall that loss functions are necessary for gradient descent\n",
    "\n",
    "$$\\sum_i{1 + Y*\\log{(\\frac{1}{1+e^{-z}})} - Y*\\log{1-(\\frac{1}{1+e^{-z}})}}$$\n",
    "$$\\sum_i{1 + Y*\\log{(\\frac{1}{1+e^{-z}})}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Take our scores and apply the sigmoid function\n",
    "    Args:\n",
    "        scores(np.array): Weighted inputs\n",
    "    Returns: \n",
    "        np.array: Sigmoid output of weighted features\n",
    "    \"\"\"\n",
    "    return(1 / (1 + np.exp(-z)))\n",
    "\n",
    "def log_likelihood(X, Y, W):\n",
    "    \"\"\"Take inputs, outputs, and weights; determine\n",
    "    the logistic log-likelihood. This will be used to understand\n",
    "    our model's progress.\n",
    "    Args:\n",
    "        X (np.array): Features/indepednent variables\n",
    "        Y (np.array): Classes/dependent variables\n",
    "        W (np.array): Model weights\n",
    "    Returns:\n",
    "        float: log-likelihood of X, Y, W\n",
    "    \"\"\"\n",
    "    z = np.dot(X, W)\n",
    "    log_likelihood = np.sum(1+Y*np.log(1/(1+np.exp(-z)))-Y*np.log(1-1/(1+np.exp(-z))))\n",
    "    return(-log_likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Function\n",
    "Putting together what we've talked about, and leveraging the support functions, we can build our own logisitc regression function.\n",
    "\n",
    "**In Class: Complete the function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X, y, num_steps, learning_rate, add_intercept=False):\n",
    "    \"\"\"Implementaiton of the logistic_regression algorithm.\n",
    "    Args:\n",
    "        X (np.array): Features/indepednent variables\n",
    "        Y (np.array): Classes/dependent variables\n",
    "        num_steps (int): Number of iterations through the training dataset\n",
    "        learning_rate (float): Learning rate for gradient descent\n",
    "        intercept (boolean): Is there an intercept term for our model\n",
    "    returns:\n",
    "        np.array: Weights for each feature/intercept\n",
    "        list: List containing all weights generated during training\n",
    "    \"\"\"\n",
    "    # Done to avoid any manipulation of the original X and y\n",
    "    lr_X = X.copy()\n",
    "    lr_y = y.copy()\n",
    "    weights_history = []\n",
    "    \n",
    "    if add_intercept:\n",
    "        intercept = np.ones((lr_X.shape[0], 1))\n",
    "        lr_X = np.hstack((intercept, lr_X))\n",
    "        W = np.random.randn(3) \n",
    "    else:\n",
    "        # Initialize our weights\n",
    "        W = np.random.randn(2)         \n",
    "\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        # Calculate the predictions for each record\n",
    "       \n",
    "        # Calculate the error term\n",
    "        \n",
    "        # Calculate the gradient\n",
    "       \n",
    "        # Update weights based on the gradient\n",
    "\n",
    "        \n",
    "        # Print log-likelihood every so often\n",
    "        if step % 100 == 0:\n",
    "            print(gradient)\n",
    "            print(log_likelihood(lr_X, lr_y, W))\n",
    "            weights_history.append(W.copy())\n",
    "            \n",
    "    if num_steps % 100 != 0:\n",
    "        weights_history.append(W.copy())\n",
    "        \n",
    "    return W, weights_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running our Model\n",
    "Once we've completed our regression function we can pass in our X and y values to evaluate how well we can train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate what weights optimize our logistic regression function\n",
    "intercept = True\n",
    "weights, history = logistic_regression(X, y, num_steps = 1000, learning_rate = .03, add_intercept=intercept)\n",
    "print(f'\\nGenerated weights:\\n{weights}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Class Exercise\n",
    "Use the given weights to draw the decision boundary for our trained model.\n",
    "\n",
    "*Hint: Logistic regression states - $p(y^i) = w_0 + w_1*x_1 + ... + w_n*x_n$. This means we can identify the inputs for a specific probability (0.5) by setting - $p(y^i) = 0.5$.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import logit\n",
    "\n",
    "colors = ['red' if x == 0 else 'blue' for x in y]\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(X[:, 0], X[:, 1], c = colors, alpha = 0.5)\n",
    "\n",
    "# Range of our X values\n",
    "start_x1 = 4\n",
    "end_x1 = 8\n",
    "\n",
    "# Change the shape of the weights depending on if there is an intercept\n",
    "if intercept:\n",
    "    w_history = history\n",
    "else:\n",
    "    w_history = np.insert(history, 0, 0, 1)\n",
    "    \n",
    "    \n",
    "a = 0\n",
    "for w in w_history[-50:-1]:\n",
    "    start_y = (w[0] + start_x1*w[1] - logit(0.5)) / -w[2]\n",
    "    end_y = (w[0] + end_x1*w[1] - logit(0.5)) / -w[2]\n",
    "    plt.plot([start_x1, end_x1], [start_y, end_y], color='grey', alpha=a)\n",
    "    plt.axis((4, 8,1.5,4.5))\n",
    "    a+=(1/len(w_history))\n",
    "    \n",
    "plt.title(\"Iris Types by Stem and Petal w/Regression\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Q1\n",
    "### Yaxni Li; Ruiyang Chen; Haoze Wang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST is a classic dataset that has become relatively easy to work with, thanks to today's advanced hardware capabilities. We trained a fully connected neural network to predict the digits, and since this type of network already achieves high accuracy, we believe that more complex networks would offer minimal improvement for this task. Therefore, there is no need to use other network types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Import the related Libs\n",
    "In this work, we use pytorch, so we import pytorch.And we also import some sub sets of it for easier code writing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Hyperparameter configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set learning rate, batch size, and number of neurons per hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "LEARNING_RATE = 0.01\n",
    "BATCH_SIZE = 64\n",
    "HIDDEN1 = 300\n",
    "HIDDEN2 = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define our fully connected NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the classical dataset, MNIST, the input size was set to 28 * 28. And we pointed out the output size should be 10, because we have 10 possible result(from 0 to 9). And the hidden layers were based on the hyperparameters we just set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNN(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(MyNN, self).__init__()\n",
    "        self.input_to_hidden1 = nn.Linear(28 * 28, HIDDEN1)\n",
    "        self.hidden1_to_hidden2 = nn.Linear(HIDDEN1, HIDDEN2)\n",
    "        self.hidden2_to_output = nn.Linear(HIDDEN2, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = F.relu(self.input_to_hidden1(x))\n",
    "        x = F.relu(self.hidden1_to_hidden2(x))\n",
    "        x = self.hidden2_to_output(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Set the tranfroms class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    # transforms.Resize(32),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5), (0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = MNIST(root = \"./\", train = True, download = True, transform = transform)\n",
    "train_loader = DataLoader(train_set, batch_size = BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MNIST('./', train = False, transform = transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 1.267\n",
      "[1,   200] loss: 0.460\n",
      "[1,   300] loss: 0.390\n",
      "[1,   400] loss: 0.332\n",
      "[1,   500] loss: 0.298\n",
      "[1,   600] loss: 0.264\n",
      "[1,   700] loss: 0.246\n",
      "[1,   800] loss: 0.226\n",
      "[1,   900] loss: 0.206\n",
      "[2,   100] loss: 0.189\n",
      "[2,   200] loss: 0.164\n",
      "[2,   300] loss: 0.176\n",
      "[2,   400] loss: 0.162\n",
      "[2,   500] loss: 0.164\n",
      "[2,   600] loss: 0.161\n",
      "[2,   700] loss: 0.146\n",
      "[2,   800] loss: 0.170\n",
      "[2,   900] loss: 0.146\n",
      "[3,   100] loss: 0.117\n",
      "[3,   200] loss: 0.114\n",
      "[3,   300] loss: 0.122\n",
      "[3,   400] loss: 0.119\n",
      "[3,   500] loss: 0.112\n",
      "[3,   600] loss: 0.111\n",
      "[3,   700] loss: 0.106\n",
      "[3,   800] loss: 0.117\n",
      "[3,   900] loss: 0.099\n",
      "[4,   100] loss: 0.092\n",
      "[4,   200] loss: 0.094\n",
      "[4,   300] loss: 0.086\n",
      "[4,   400] loss: 0.084\n",
      "[4,   500] loss: 0.095\n",
      "[4,   600] loss: 0.091\n",
      "[4,   700] loss: 0.080\n",
      "[4,   800] loss: 0.092\n",
      "[4,   900] loss: 0.091\n",
      "[5,   100] loss: 0.060\n",
      "[5,   200] loss: 0.073\n",
      "[5,   300] loss: 0.067\n",
      "[5,   400] loss: 0.074\n",
      "[5,   500] loss: 0.068\n",
      "[5,   600] loss: 0.089\n",
      "[5,   700] loss: 0.086\n",
      "[5,   800] loss: 0.075\n",
      "[5,   900] loss: 0.067\n"
     ]
    }
   ],
   "source": [
    "model = MyNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = LEARNING_RATE, momentum = 0.9)\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, laberls = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, laberls)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.Test our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test code print the accurate rate on console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the network on the test set: 97 %\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('The accuracy of the network on the test set: %d %%' % (100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
